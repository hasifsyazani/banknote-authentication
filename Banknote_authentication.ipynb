{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "banknote_datadset = pd.read_csv('banknote_authentication.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skew</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variance    skew  curtosis  entropy  class\n",
       "0   3.62160  8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590  8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600 -2.6383    1.9242  0.10645      0\n",
       "3   3.45660  9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924 -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banknote_datadset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skew</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "      <td>1372.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.433735</td>\n",
       "      <td>1.922353</td>\n",
       "      <td>1.397627</td>\n",
       "      <td>-1.191657</td>\n",
       "      <td>0.444606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.842763</td>\n",
       "      <td>5.869047</td>\n",
       "      <td>4.310030</td>\n",
       "      <td>2.101013</td>\n",
       "      <td>0.497103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-7.042100</td>\n",
       "      <td>-13.773100</td>\n",
       "      <td>-5.286100</td>\n",
       "      <td>-8.548200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.773000</td>\n",
       "      <td>-1.708200</td>\n",
       "      <td>-1.574975</td>\n",
       "      <td>-2.413450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.496180</td>\n",
       "      <td>2.319650</td>\n",
       "      <td>0.616630</td>\n",
       "      <td>-0.586650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.821475</td>\n",
       "      <td>6.814625</td>\n",
       "      <td>3.179250</td>\n",
       "      <td>0.394810</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.824800</td>\n",
       "      <td>12.951600</td>\n",
       "      <td>17.927400</td>\n",
       "      <td>2.449500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          variance         skew     curtosis      entropy        class\n",
       "count  1372.000000  1372.000000  1372.000000  1372.000000  1372.000000\n",
       "mean      0.433735     1.922353     1.397627    -1.191657     0.444606\n",
       "std       2.842763     5.869047     4.310030     2.101013     0.497103\n",
       "min      -7.042100   -13.773100    -5.286100    -8.548200     0.000000\n",
       "25%      -1.773000    -1.708200    -1.574975    -2.413450     0.000000\n",
       "50%       0.496180     2.319650     0.616630    -0.586650     0.000000\n",
       "75%       2.821475     6.814625     3.179250     0.394810     1.000000\n",
       "max       6.824800    12.951600    17.927400     2.449500     1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banknote_datadset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features = banknote_datadset.iloc[:, 0:4].values \n",
    "dataset_labels = banknote_datadset.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(dataset_features, dataset_labels, test_size=0.3, random_state=0) \n",
    "rfc_object = rfc(n_estimators=200, random_state=0) \n",
    "rfc_object.fit(train_features, train_labels)\n",
    "predicted_labels = rfc_object.predict(test_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set - Inputs : \n",
      " [[-1.4572    9.1214    1.7425   -5.1241  ]\n",
      " [-4.0786    2.9239    0.87026  -0.65389 ]\n",
      " [-0.90784  -7.9026    6.7807    0.34179 ]\n",
      " ...\n",
      " [ 0.6005    0.99945  -2.2126    0.097399]\n",
      " [ 2.0165   -0.25246   5.1707    1.0763  ]\n",
      " [-2.0759   10.8223    2.6439   -4.837   ]] \n",
      "\n",
      "Training set - Labels/Outputs : \n",
      " [0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0\n",
      " 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
      " 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1\n",
      " 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1\n",
      " 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0\n",
      " 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1\n",
      " 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
      " 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0\n",
      " 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
      " 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0\n",
      " 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1\n",
      " 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0\n",
      " 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0] \n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       232\n",
      "           1       0.98      0.99      0.99       180\n",
      "\n",
      "    accuracy                           0.99       412\n",
      "   macro avg       0.99      0.99      0.99       412\n",
      "weighted avg       0.99      0.99      0.99       412\n",
      "\n",
      "Confusion matrix :\n",
      " [[229   3]\n",
      " [  1 179]] \n",
      "\n",
      "Accuracy :\n",
      " 0.9902912621359223\n"
     ]
    }
   ],
   "source": [
    "x = train_features\n",
    "print(\"Training set - Inputs : \\n\",x,\"\\n\")\n",
    "y = train_labels\n",
    "print(\"Training set - Labels/Outputs : \\n\",y,\"\\n\")\n",
    "\n",
    "print(\"Classification report : \\n\",classification_report(test_labels, predicted_labels)) \n",
    "\n",
    "print(\"Confusion matrix :\\n\",confusion_matrix(test_labels, predicted_labels),\"\\n\") \n",
    "\n",
    "print(\"Accuracy :\\n\",accuracy_score(test_labels, predicted_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=0, lrate=0.500, error=0.679\n",
      ">epoch=1, lrate=0.500, error=0.594\n",
      ">epoch=2, lrate=0.500, error=0.586\n",
      ">epoch=3, lrate=0.500, error=0.582\n",
      ">epoch=4, lrate=0.500, error=0.580\n",
      ">epoch=5, lrate=0.500, error=0.578\n",
      ">epoch=6, lrate=0.500, error=0.577\n",
      ">epoch=7, lrate=0.500, error=0.575\n",
      ">epoch=8, lrate=0.500, error=0.574\n",
      ">epoch=9, lrate=0.500, error=0.573\n",
      ">epoch=10, lrate=0.500, error=0.571\n",
      ">epoch=11, lrate=0.500, error=0.568\n",
      ">epoch=12, lrate=0.500, error=0.566\n",
      ">epoch=13, lrate=0.500, error=0.564\n",
      ">epoch=14, lrate=0.500, error=0.563\n",
      ">epoch=15, lrate=0.500, error=0.562\n",
      ">epoch=16, lrate=0.500, error=0.562\n",
      ">epoch=17, lrate=0.500, error=0.561\n",
      ">epoch=18, lrate=0.500, error=0.560\n",
      ">epoch=19, lrate=0.500, error=0.560\n",
      ">epoch=20, lrate=0.500, error=0.559\n",
      ">epoch=21, lrate=0.500, error=0.559\n",
      ">epoch=22, lrate=0.500, error=0.558\n",
      ">epoch=23, lrate=0.500, error=0.558\n",
      ">epoch=24, lrate=0.500, error=0.558\n",
      ">epoch=25, lrate=0.500, error=0.558\n",
      ">epoch=26, lrate=0.500, error=0.558\n",
      ">epoch=27, lrate=0.500, error=0.558\n",
      ">epoch=28, lrate=0.500, error=0.558\n",
      ">epoch=29, lrate=0.500, error=0.558\n",
      ">epoch=30, lrate=0.500, error=0.557\n",
      ">epoch=31, lrate=0.500, error=0.557\n",
      ">epoch=32, lrate=0.500, error=0.557\n",
      ">epoch=33, lrate=0.500, error=0.557\n",
      ">epoch=34, lrate=0.500, error=0.557\n",
      ">epoch=35, lrate=0.500, error=0.557\n",
      ">epoch=36, lrate=0.500, error=0.557\n",
      ">epoch=37, lrate=0.500, error=0.557\n",
      ">epoch=38, lrate=0.500, error=0.557\n",
      ">epoch=39, lrate=0.500, error=0.557\n",
      ">epoch=40, lrate=0.500, error=0.557\n",
      ">epoch=41, lrate=0.500, error=0.557\n",
      ">epoch=42, lrate=0.500, error=0.556\n",
      ">epoch=43, lrate=0.500, error=0.556\n",
      ">epoch=44, lrate=0.500, error=0.556\n",
      ">epoch=45, lrate=0.500, error=0.556\n",
      ">epoch=46, lrate=0.500, error=0.556\n",
      ">epoch=47, lrate=0.500, error=0.556\n",
      ">epoch=48, lrate=0.500, error=0.556\n",
      ">epoch=49, lrate=0.500, error=0.556\n",
      ">epoch=50, lrate=0.500, error=0.556\n",
      ">epoch=51, lrate=0.500, error=0.556\n",
      ">epoch=52, lrate=0.500, error=0.556\n",
      ">epoch=53, lrate=0.500, error=0.556\n",
      ">epoch=54, lrate=0.500, error=0.556\n",
      ">epoch=55, lrate=0.500, error=0.556\n",
      ">epoch=56, lrate=0.500, error=0.556\n",
      ">epoch=57, lrate=0.500, error=0.556\n",
      ">epoch=58, lrate=0.500, error=0.556\n",
      ">epoch=59, lrate=0.500, error=0.556\n",
      ">epoch=60, lrate=0.500, error=0.556\n",
      ">epoch=61, lrate=0.500, error=0.556\n",
      ">epoch=62, lrate=0.500, error=0.556\n",
      ">epoch=63, lrate=0.500, error=0.556\n",
      ">epoch=64, lrate=0.500, error=0.556\n",
      ">epoch=65, lrate=0.500, error=0.556\n",
      ">epoch=66, lrate=0.500, error=0.556\n",
      ">epoch=67, lrate=0.500, error=0.556\n",
      ">epoch=68, lrate=0.500, error=0.556\n",
      ">epoch=69, lrate=0.500, error=0.556\n",
      ">epoch=70, lrate=0.500, error=0.556\n",
      ">epoch=71, lrate=0.500, error=0.556\n",
      ">epoch=72, lrate=0.500, error=0.556\n",
      ">epoch=73, lrate=0.500, error=0.556\n",
      ">epoch=74, lrate=0.500, error=0.556\n",
      ">epoch=75, lrate=0.500, error=0.556\n",
      ">epoch=76, lrate=0.500, error=0.556\n",
      ">epoch=77, lrate=0.500, error=0.556\n",
      ">epoch=78, lrate=0.500, error=0.556\n",
      ">epoch=79, lrate=0.500, error=0.556\n",
      ">epoch=80, lrate=0.500, error=0.556\n",
      ">epoch=81, lrate=0.500, error=0.556\n",
      ">epoch=82, lrate=0.500, error=0.556\n",
      ">epoch=83, lrate=0.500, error=0.556\n",
      ">epoch=84, lrate=0.500, error=0.556\n",
      ">epoch=85, lrate=0.500, error=0.556\n",
      ">epoch=86, lrate=0.500, error=0.556\n",
      ">epoch=87, lrate=0.500, error=0.555\n",
      ">epoch=88, lrate=0.500, error=0.555\n",
      ">epoch=89, lrate=0.500, error=0.555\n",
      ">epoch=90, lrate=0.500, error=0.555\n",
      ">epoch=91, lrate=0.500, error=0.555\n",
      ">epoch=92, lrate=0.500, error=0.555\n",
      ">epoch=93, lrate=0.500, error=0.555\n",
      ">epoch=94, lrate=0.500, error=0.555\n",
      ">epoch=95, lrate=0.500, error=0.555\n",
      ">epoch=96, lrate=0.500, error=0.555\n",
      ">epoch=97, lrate=0.500, error=0.555\n",
      ">epoch=98, lrate=0.500, error=0.555\n",
      ">epoch=99, lrate=0.500, error=0.555\n",
      ">epoch=100, lrate=0.500, error=0.555\n",
      ">epoch=101, lrate=0.500, error=0.555\n",
      ">epoch=102, lrate=0.500, error=0.555\n",
      ">epoch=103, lrate=0.500, error=0.555\n",
      ">epoch=104, lrate=0.500, error=0.555\n",
      ">epoch=105, lrate=0.500, error=0.555\n",
      ">epoch=106, lrate=0.500, error=0.555\n",
      ">epoch=107, lrate=0.500, error=0.555\n",
      ">epoch=108, lrate=0.500, error=0.555\n",
      ">epoch=109, lrate=0.500, error=0.555\n",
      ">epoch=110, lrate=0.500, error=0.554\n",
      ">epoch=111, lrate=0.500, error=0.554\n",
      ">epoch=112, lrate=0.500, error=0.555\n",
      ">epoch=113, lrate=0.500, error=0.555\n",
      ">epoch=114, lrate=0.500, error=0.555\n",
      ">epoch=115, lrate=0.500, error=0.555\n",
      ">epoch=116, lrate=0.500, error=0.555\n",
      ">epoch=117, lrate=0.500, error=0.555\n",
      ">epoch=118, lrate=0.500, error=0.555\n",
      ">epoch=119, lrate=0.500, error=0.555\n",
      ">epoch=120, lrate=0.500, error=0.555\n",
      ">epoch=121, lrate=0.500, error=0.555\n",
      ">epoch=122, lrate=0.500, error=0.556\n",
      ">epoch=123, lrate=0.500, error=0.555\n",
      ">epoch=124, lrate=0.500, error=0.555\n",
      ">epoch=125, lrate=0.500, error=0.555\n",
      ">epoch=126, lrate=0.500, error=0.555\n",
      ">epoch=127, lrate=0.500, error=0.555\n",
      ">epoch=128, lrate=0.500, error=0.555\n",
      ">epoch=129, lrate=0.500, error=0.554\n",
      ">epoch=130, lrate=0.500, error=0.554\n",
      ">epoch=131, lrate=0.500, error=0.554\n",
      ">epoch=132, lrate=0.500, error=0.554\n",
      ">epoch=133, lrate=0.500, error=0.554\n",
      ">epoch=134, lrate=0.500, error=0.554\n",
      ">epoch=135, lrate=0.500, error=0.554\n",
      ">epoch=136, lrate=0.500, error=0.554\n",
      ">epoch=137, lrate=0.500, error=0.554\n",
      ">epoch=138, lrate=0.500, error=0.554\n",
      ">epoch=139, lrate=0.500, error=0.554\n",
      ">epoch=140, lrate=0.500, error=0.554\n",
      ">epoch=141, lrate=0.500, error=0.554\n",
      ">epoch=142, lrate=0.500, error=0.554\n",
      ">epoch=143, lrate=0.500, error=0.554\n",
      ">epoch=144, lrate=0.500, error=0.554\n",
      ">epoch=145, lrate=0.500, error=0.554\n",
      ">epoch=146, lrate=0.500, error=0.554\n",
      ">epoch=147, lrate=0.500, error=0.554\n",
      ">epoch=148, lrate=0.500, error=0.554\n",
      ">epoch=149, lrate=0.500, error=0.554\n",
      ">epoch=150, lrate=0.500, error=0.553\n",
      ">epoch=151, lrate=0.500, error=0.554\n",
      ">epoch=152, lrate=0.500, error=0.553\n",
      ">epoch=153, lrate=0.500, error=0.553\n",
      ">epoch=154, lrate=0.500, error=0.553\n",
      ">epoch=155, lrate=0.500, error=0.553\n",
      ">epoch=156, lrate=0.500, error=0.553\n",
      ">epoch=157, lrate=0.500, error=0.553\n",
      ">epoch=158, lrate=0.500, error=0.553\n",
      ">epoch=159, lrate=0.500, error=0.553\n",
      ">epoch=160, lrate=0.500, error=0.553\n",
      ">epoch=161, lrate=0.500, error=0.553\n",
      ">epoch=162, lrate=0.500, error=0.553\n",
      ">epoch=163, lrate=0.500, error=0.553\n",
      ">epoch=164, lrate=0.500, error=0.553\n",
      ">epoch=165, lrate=0.500, error=0.553\n",
      ">epoch=166, lrate=0.500, error=0.553\n",
      ">epoch=167, lrate=0.500, error=0.553\n",
      ">epoch=168, lrate=0.500, error=0.553\n",
      ">epoch=169, lrate=0.500, error=0.553\n",
      ">epoch=170, lrate=0.500, error=0.553\n",
      ">epoch=171, lrate=0.500, error=0.553\n",
      ">epoch=172, lrate=0.500, error=0.553\n",
      ">epoch=173, lrate=0.500, error=0.553\n",
      ">epoch=174, lrate=0.500, error=0.553\n",
      ">epoch=175, lrate=0.500, error=0.553\n",
      ">epoch=176, lrate=0.500, error=0.553\n",
      ">epoch=177, lrate=0.500, error=0.553\n",
      ">epoch=178, lrate=0.500, error=0.553\n",
      ">epoch=179, lrate=0.500, error=0.553\n",
      ">epoch=180, lrate=0.500, error=0.553\n",
      ">epoch=181, lrate=0.500, error=0.553\n",
      ">epoch=182, lrate=0.500, error=0.553\n",
      ">epoch=183, lrate=0.500, error=0.553\n",
      ">epoch=184, lrate=0.500, error=0.553\n",
      ">epoch=185, lrate=0.500, error=0.553\n",
      ">epoch=186, lrate=0.500, error=0.553\n",
      ">epoch=187, lrate=0.500, error=0.553\n",
      ">epoch=188, lrate=0.500, error=0.553\n",
      ">epoch=189, lrate=0.500, error=0.553\n",
      ">epoch=190, lrate=0.500, error=0.553\n",
      ">epoch=191, lrate=0.500, error=0.553\n",
      ">epoch=192, lrate=0.500, error=0.553\n",
      ">epoch=193, lrate=0.500, error=0.553\n",
      ">epoch=194, lrate=0.500, error=0.553\n",
      ">epoch=195, lrate=0.500, error=0.553\n",
      ">epoch=196, lrate=0.500, error=0.553\n",
      ">epoch=197, lrate=0.500, error=0.553\n",
      ">epoch=198, lrate=0.500, error=0.553\n",
      ">epoch=199, lrate=0.500, error=0.553\n",
      ">epoch=200, lrate=0.500, error=0.553\n",
      ">epoch=201, lrate=0.500, error=0.553\n",
      ">epoch=202, lrate=0.500, error=0.553\n",
      ">epoch=203, lrate=0.500, error=0.553\n",
      ">epoch=204, lrate=0.500, error=0.553\n",
      ">epoch=205, lrate=0.500, error=0.553\n",
      ">epoch=206, lrate=0.500, error=0.553\n",
      ">epoch=207, lrate=0.500, error=0.553\n",
      ">epoch=208, lrate=0.500, error=0.553\n",
      ">epoch=209, lrate=0.500, error=0.553\n",
      ">epoch=210, lrate=0.500, error=0.553\n",
      ">epoch=211, lrate=0.500, error=0.553\n",
      ">epoch=212, lrate=0.500, error=0.553\n",
      ">epoch=213, lrate=0.500, error=0.553\n",
      ">epoch=214, lrate=0.500, error=0.553\n",
      ">epoch=215, lrate=0.500, error=0.553\n",
      ">epoch=216, lrate=0.500, error=0.553\n",
      ">epoch=217, lrate=0.500, error=0.553\n",
      ">epoch=218, lrate=0.500, error=0.553\n",
      ">epoch=219, lrate=0.500, error=0.553\n",
      ">epoch=220, lrate=0.500, error=0.553\n",
      ">epoch=221, lrate=0.500, error=0.553\n",
      ">epoch=222, lrate=0.500, error=0.553\n",
      ">epoch=223, lrate=0.500, error=0.553\n",
      ">epoch=224, lrate=0.500, error=0.553\n",
      ">epoch=225, lrate=0.500, error=0.553\n",
      ">epoch=226, lrate=0.500, error=0.553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=227, lrate=0.500, error=0.553\n",
      ">epoch=228, lrate=0.500, error=0.553\n",
      ">epoch=229, lrate=0.500, error=0.553\n",
      ">epoch=230, lrate=0.500, error=0.553\n",
      ">epoch=231, lrate=0.500, error=0.553\n",
      ">epoch=232, lrate=0.500, error=0.553\n",
      ">epoch=233, lrate=0.500, error=0.553\n",
      ">epoch=234, lrate=0.500, error=0.553\n",
      ">epoch=235, lrate=0.500, error=0.553\n",
      ">epoch=236, lrate=0.500, error=0.553\n",
      ">epoch=237, lrate=0.500, error=0.553\n",
      ">epoch=238, lrate=0.500, error=0.553\n",
      ">epoch=239, lrate=0.500, error=0.553\n",
      ">epoch=240, lrate=0.500, error=0.553\n",
      ">epoch=241, lrate=0.500, error=0.553\n",
      ">epoch=242, lrate=0.500, error=0.553\n",
      ">epoch=243, lrate=0.500, error=0.553\n",
      ">epoch=244, lrate=0.500, error=0.553\n",
      ">epoch=245, lrate=0.500, error=0.553\n",
      ">epoch=246, lrate=0.500, error=0.553\n",
      ">epoch=247, lrate=0.500, error=0.553\n",
      ">epoch=248, lrate=0.500, error=0.553\n",
      ">epoch=249, lrate=0.500, error=0.553\n",
      ">epoch=250, lrate=0.500, error=0.553\n",
      ">epoch=251, lrate=0.500, error=0.553\n",
      ">epoch=252, lrate=0.500, error=0.552\n",
      ">epoch=253, lrate=0.500, error=0.552\n",
      ">epoch=254, lrate=0.500, error=0.552\n",
      ">epoch=255, lrate=0.500, error=0.552\n",
      ">epoch=256, lrate=0.500, error=0.552\n",
      ">epoch=257, lrate=0.500, error=0.552\n",
      ">epoch=258, lrate=0.500, error=0.552\n",
      ">epoch=259, lrate=0.500, error=0.552\n",
      ">epoch=260, lrate=0.500, error=0.552\n",
      ">epoch=261, lrate=0.500, error=0.552\n",
      ">epoch=262, lrate=0.500, error=0.552\n",
      ">epoch=263, lrate=0.500, error=0.552\n",
      ">epoch=264, lrate=0.500, error=0.552\n",
      ">epoch=265, lrate=0.500, error=0.552\n",
      ">epoch=266, lrate=0.500, error=0.552\n",
      ">epoch=267, lrate=0.500, error=0.552\n",
      ">epoch=268, lrate=0.500, error=0.552\n",
      ">epoch=269, lrate=0.500, error=0.552\n",
      ">epoch=270, lrate=0.500, error=0.552\n",
      ">epoch=271, lrate=0.500, error=0.552\n",
      ">epoch=272, lrate=0.500, error=0.552\n",
      ">epoch=273, lrate=0.500, error=0.552\n",
      ">epoch=274, lrate=0.500, error=0.552\n",
      ">epoch=275, lrate=0.500, error=0.552\n",
      ">epoch=276, lrate=0.500, error=0.552\n",
      ">epoch=277, lrate=0.500, error=0.552\n",
      ">epoch=278, lrate=0.500, error=0.552\n",
      ">epoch=279, lrate=0.500, error=0.552\n",
      ">epoch=280, lrate=0.500, error=0.552\n",
      ">epoch=281, lrate=0.500, error=0.552\n",
      ">epoch=282, lrate=0.500, error=0.552\n",
      ">epoch=283, lrate=0.500, error=0.552\n",
      ">epoch=284, lrate=0.500, error=0.552\n",
      ">epoch=285, lrate=0.500, error=0.552\n",
      ">epoch=286, lrate=0.500, error=0.552\n",
      ">epoch=287, lrate=0.500, error=0.552\n",
      ">epoch=288, lrate=0.500, error=0.552\n",
      ">epoch=289, lrate=0.500, error=0.552\n",
      ">epoch=290, lrate=0.500, error=0.552\n",
      ">epoch=291, lrate=0.500, error=0.552\n",
      ">epoch=292, lrate=0.500, error=0.552\n",
      ">epoch=293, lrate=0.500, error=0.552\n",
      ">epoch=294, lrate=0.500, error=0.552\n",
      ">epoch=295, lrate=0.500, error=0.552\n",
      ">epoch=296, lrate=0.500, error=0.552\n",
      ">epoch=297, lrate=0.500, error=0.552\n",
      ">epoch=298, lrate=0.500, error=0.552\n",
      ">epoch=299, lrate=0.500, error=0.552\n",
      ">epoch=300, lrate=0.500, error=0.552\n",
      ">epoch=301, lrate=0.500, error=0.552\n",
      ">epoch=302, lrate=0.500, error=0.552\n",
      ">epoch=303, lrate=0.500, error=0.552\n",
      ">epoch=304, lrate=0.500, error=0.552\n",
      ">epoch=305, lrate=0.500, error=0.552\n",
      ">epoch=306, lrate=0.500, error=0.552\n",
      ">epoch=307, lrate=0.500, error=0.552\n",
      ">epoch=308, lrate=0.500, error=0.552\n",
      ">epoch=309, lrate=0.500, error=0.552\n",
      ">epoch=310, lrate=0.500, error=0.552\n",
      ">epoch=311, lrate=0.500, error=0.552\n",
      ">epoch=312, lrate=0.500, error=0.552\n",
      ">epoch=313, lrate=0.500, error=0.552\n",
      ">epoch=314, lrate=0.500, error=0.552\n",
      ">epoch=315, lrate=0.500, error=0.552\n",
      ">epoch=316, lrate=0.500, error=0.552\n",
      ">epoch=317, lrate=0.500, error=0.552\n",
      ">epoch=318, lrate=0.500, error=0.552\n",
      ">epoch=319, lrate=0.500, error=0.552\n",
      ">epoch=320, lrate=0.500, error=0.552\n",
      ">epoch=321, lrate=0.500, error=0.552\n",
      ">epoch=322, lrate=0.500, error=0.552\n",
      ">epoch=323, lrate=0.500, error=0.552\n",
      ">epoch=324, lrate=0.500, error=0.552\n",
      ">epoch=325, lrate=0.500, error=0.552\n",
      ">epoch=326, lrate=0.500, error=0.552\n",
      ">epoch=327, lrate=0.500, error=0.552\n",
      ">epoch=328, lrate=0.500, error=0.552\n",
      ">epoch=329, lrate=0.500, error=0.552\n",
      ">epoch=330, lrate=0.500, error=0.552\n",
      ">epoch=331, lrate=0.500, error=0.552\n",
      ">epoch=332, lrate=0.500, error=0.552\n",
      ">epoch=333, lrate=0.500, error=0.552\n",
      ">epoch=334, lrate=0.500, error=0.552\n",
      ">epoch=335, lrate=0.500, error=0.552\n",
      ">epoch=336, lrate=0.500, error=0.552\n",
      ">epoch=337, lrate=0.500, error=0.552\n",
      ">epoch=338, lrate=0.500, error=0.552\n",
      ">epoch=339, lrate=0.500, error=0.552\n",
      ">epoch=340, lrate=0.500, error=0.552\n",
      ">epoch=341, lrate=0.500, error=0.552\n",
      ">epoch=342, lrate=0.500, error=0.552\n",
      ">epoch=343, lrate=0.500, error=0.552\n",
      ">epoch=344, lrate=0.500, error=0.552\n",
      ">epoch=345, lrate=0.500, error=0.552\n",
      ">epoch=346, lrate=0.500, error=0.552\n",
      ">epoch=347, lrate=0.500, error=0.552\n",
      ">epoch=348, lrate=0.500, error=0.552\n",
      ">epoch=349, lrate=0.500, error=0.552\n",
      ">epoch=350, lrate=0.500, error=0.552\n",
      ">epoch=351, lrate=0.500, error=0.552\n",
      ">epoch=352, lrate=0.500, error=0.552\n",
      ">epoch=353, lrate=0.500, error=0.552\n",
      ">epoch=354, lrate=0.500, error=0.552\n",
      ">epoch=355, lrate=0.500, error=0.552\n",
      ">epoch=356, lrate=0.500, error=0.552\n",
      ">epoch=357, lrate=0.500, error=0.552\n",
      ">epoch=358, lrate=0.500, error=0.552\n",
      ">epoch=359, lrate=0.500, error=0.552\n",
      ">epoch=360, lrate=0.500, error=0.552\n",
      ">epoch=361, lrate=0.500, error=0.552\n",
      ">epoch=362, lrate=0.500, error=0.552\n",
      ">epoch=363, lrate=0.500, error=0.552\n",
      ">epoch=364, lrate=0.500, error=0.552\n",
      ">epoch=365, lrate=0.500, error=0.552\n",
      ">epoch=366, lrate=0.500, error=0.552\n",
      ">epoch=367, lrate=0.500, error=0.552\n",
      ">epoch=368, lrate=0.500, error=0.552\n",
      ">epoch=369, lrate=0.500, error=0.552\n",
      ">epoch=370, lrate=0.500, error=0.552\n",
      ">epoch=371, lrate=0.500, error=0.552\n",
      ">epoch=372, lrate=0.500, error=0.552\n",
      ">epoch=373, lrate=0.500, error=0.552\n",
      ">epoch=374, lrate=0.500, error=0.552\n",
      ">epoch=375, lrate=0.500, error=0.552\n",
      ">epoch=376, lrate=0.500, error=0.552\n",
      ">epoch=377, lrate=0.500, error=0.552\n",
      ">epoch=378, lrate=0.500, error=0.552\n",
      ">epoch=379, lrate=0.500, error=0.552\n",
      ">epoch=380, lrate=0.500, error=0.552\n",
      ">epoch=381, lrate=0.500, error=0.552\n",
      ">epoch=382, lrate=0.500, error=0.552\n",
      ">epoch=383, lrate=0.500, error=0.552\n",
      ">epoch=384, lrate=0.500, error=0.552\n",
      ">epoch=385, lrate=0.500, error=0.552\n",
      ">epoch=386, lrate=0.500, error=0.552\n",
      ">epoch=387, lrate=0.500, error=0.552\n",
      ">epoch=388, lrate=0.500, error=0.552\n",
      ">epoch=389, lrate=0.500, error=0.552\n",
      ">epoch=390, lrate=0.500, error=0.552\n",
      ">epoch=391, lrate=0.500, error=0.552\n",
      ">epoch=392, lrate=0.500, error=0.552\n",
      ">epoch=393, lrate=0.500, error=0.552\n",
      ">epoch=394, lrate=0.500, error=0.552\n",
      ">epoch=395, lrate=0.500, error=0.552\n",
      ">epoch=396, lrate=0.500, error=0.552\n",
      ">epoch=397, lrate=0.500, error=0.552\n",
      ">epoch=398, lrate=0.500, error=0.552\n",
      ">epoch=399, lrate=0.500, error=0.552\n",
      ">epoch=400, lrate=0.500, error=0.552\n",
      ">epoch=401, lrate=0.500, error=0.552\n",
      ">epoch=402, lrate=0.500, error=0.552\n",
      ">epoch=403, lrate=0.500, error=0.552\n",
      ">epoch=404, lrate=0.500, error=0.552\n",
      ">epoch=405, lrate=0.500, error=0.552\n",
      ">epoch=406, lrate=0.500, error=0.552\n",
      ">epoch=407, lrate=0.500, error=0.552\n",
      ">epoch=408, lrate=0.500, error=0.552\n",
      ">epoch=409, lrate=0.500, error=0.552\n",
      ">epoch=410, lrate=0.500, error=0.552\n",
      ">epoch=411, lrate=0.500, error=0.552\n",
      ">epoch=412, lrate=0.500, error=0.552\n",
      ">epoch=413, lrate=0.500, error=0.552\n",
      ">epoch=414, lrate=0.500, error=0.552\n",
      ">epoch=415, lrate=0.500, error=0.552\n",
      ">epoch=416, lrate=0.500, error=0.552\n",
      ">epoch=417, lrate=0.500, error=0.552\n",
      ">epoch=418, lrate=0.500, error=0.552\n",
      ">epoch=419, lrate=0.500, error=0.552\n",
      ">epoch=420, lrate=0.500, error=0.552\n",
      ">epoch=421, lrate=0.500, error=0.552\n",
      ">epoch=422, lrate=0.500, error=0.552\n",
      ">epoch=423, lrate=0.500, error=0.552\n",
      ">epoch=424, lrate=0.500, error=0.552\n",
      ">epoch=425, lrate=0.500, error=0.552\n",
      ">epoch=426, lrate=0.500, error=0.552\n",
      ">epoch=427, lrate=0.500, error=0.552\n",
      ">epoch=428, lrate=0.500, error=0.552\n",
      ">epoch=429, lrate=0.500, error=0.552\n",
      ">epoch=430, lrate=0.500, error=0.552\n",
      ">epoch=431, lrate=0.500, error=0.552\n",
      ">epoch=432, lrate=0.500, error=0.552\n",
      ">epoch=433, lrate=0.500, error=0.552\n",
      ">epoch=434, lrate=0.500, error=0.552\n",
      ">epoch=435, lrate=0.500, error=0.552\n",
      ">epoch=436, lrate=0.500, error=0.552\n",
      ">epoch=437, lrate=0.500, error=0.552\n",
      ">epoch=438, lrate=0.500, error=0.552\n",
      ">epoch=439, lrate=0.500, error=0.552\n",
      ">epoch=440, lrate=0.500, error=0.552\n",
      ">epoch=441, lrate=0.500, error=0.552\n",
      ">epoch=442, lrate=0.500, error=0.552\n",
      ">epoch=443, lrate=0.500, error=0.552\n",
      ">epoch=444, lrate=0.500, error=0.552\n",
      ">epoch=445, lrate=0.500, error=0.552\n",
      ">epoch=446, lrate=0.500, error=0.552\n",
      ">epoch=447, lrate=0.500, error=0.552\n",
      ">epoch=448, lrate=0.500, error=0.552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch=449, lrate=0.500, error=0.552\n",
      ">epoch=450, lrate=0.500, error=0.552\n",
      ">epoch=451, lrate=0.500, error=0.552\n",
      ">epoch=452, lrate=0.500, error=0.552\n",
      ">epoch=453, lrate=0.500, error=0.552\n",
      ">epoch=454, lrate=0.500, error=0.552\n",
      ">epoch=455, lrate=0.500, error=0.552\n",
      ">epoch=456, lrate=0.500, error=0.552\n",
      ">epoch=457, lrate=0.500, error=0.552\n",
      ">epoch=458, lrate=0.500, error=0.552\n",
      ">epoch=459, lrate=0.500, error=0.552\n",
      ">epoch=460, lrate=0.500, error=0.552\n",
      ">epoch=461, lrate=0.500, error=0.552\n",
      ">epoch=462, lrate=0.500, error=0.552\n",
      ">epoch=463, lrate=0.500, error=0.552\n",
      ">epoch=464, lrate=0.500, error=0.552\n",
      ">epoch=465, lrate=0.500, error=0.552\n",
      ">epoch=466, lrate=0.500, error=0.552\n",
      ">epoch=467, lrate=0.500, error=0.552\n",
      ">epoch=468, lrate=0.500, error=0.552\n",
      ">epoch=469, lrate=0.500, error=0.552\n",
      ">epoch=470, lrate=0.500, error=0.552\n",
      ">epoch=471, lrate=0.500, error=0.552\n",
      ">epoch=472, lrate=0.500, error=0.552\n",
      ">epoch=473, lrate=0.500, error=0.552\n",
      ">epoch=474, lrate=0.500, error=0.552\n",
      ">epoch=475, lrate=0.500, error=0.552\n",
      ">epoch=476, lrate=0.500, error=0.552\n",
      ">epoch=477, lrate=0.500, error=0.552\n",
      ">epoch=478, lrate=0.500, error=0.551\n",
      ">epoch=479, lrate=0.500, error=0.551\n",
      ">epoch=480, lrate=0.500, error=0.551\n",
      ">epoch=481, lrate=0.500, error=0.551\n",
      ">epoch=482, lrate=0.500, error=0.551\n",
      ">epoch=483, lrate=0.500, error=0.551\n",
      ">epoch=484, lrate=0.500, error=0.551\n",
      ">epoch=485, lrate=0.500, error=0.551\n",
      ">epoch=486, lrate=0.500, error=0.551\n",
      ">epoch=487, lrate=0.500, error=0.551\n",
      ">epoch=488, lrate=0.500, error=0.551\n",
      ">epoch=489, lrate=0.500, error=0.551\n",
      ">epoch=490, lrate=0.500, error=0.551\n",
      ">epoch=491, lrate=0.500, error=0.551\n",
      ">epoch=492, lrate=0.500, error=0.551\n",
      ">epoch=493, lrate=0.500, error=0.551\n",
      ">epoch=494, lrate=0.500, error=0.551\n",
      ">epoch=495, lrate=0.500, error=0.551\n",
      ">epoch=496, lrate=0.500, error=0.551\n",
      ">epoch=497, lrate=0.500, error=0.551\n",
      ">epoch=498, lrate=0.500, error=0.551\n",
      ">epoch=499, lrate=0.500, error=0.551\n",
      "Accuracy : 55.208333333333336\n"
     ]
    }
   ],
   "source": [
    "from math import exp\n",
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "# MinMax\n",
    "def dataset_minmax(dataset):\n",
    "    minmax = list()\n",
    "    stats = [[min(column), max(column)] for column in zip(*dataset)]\n",
    "    return stats\n",
    "\n",
    "# Normalization\n",
    "def normalize_dataset(dataset, minmax):\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)-1):\n",
    "            row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    " \n",
    "# Initialize a network\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    " \n",
    "# Calculate neuron activation for an input\n",
    "def activate(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    " \n",
    "# Transfer neuron activation\n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    " \n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    " \n",
    "# Calculate the derivative of an neuron output\n",
    "def transfer_derivative(output):\n",
    "    return output * (1.0 - output)\n",
    " \n",
    "# Backpropagate error and store in neurons\n",
    "def backward_propagate_error(network, expected):\n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        if i != len(network)-1:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0.0\n",
    "                for neuron in network[i + 1]:\n",
    "                    error += (neuron['weights'][j] * neuron['delta'])\n",
    "                errors.append(error)\n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                neuron = layer[j]\n",
    "                errors.append(expected[j] - neuron['output'])\n",
    "        for j in range(len(layer)):\n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    " \n",
    "# Update network weights with error\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    " \n",
    "# Train a network for a fixed number of epochs\n",
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1]] = 1\n",
    "            sum_error += sum([(expected[i]-outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate_error(network, expected)\n",
    "            update_weights(network, row, l_rate)\n",
    "        print('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error/1000.0))\n",
    "\n",
    "# Make a prediction with a network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))        \n",
    "\n",
    "# Calculate accuracy \n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Test training backprop algorithm\n",
    "seed(1)\n",
    "banktrain = x.astype(np.int64)\n",
    "n_inputs = len(banktrain[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in banktrain]))\n",
    "network = initialize_network(n_inputs, 3, n_outputs)\n",
    "train_network(network, banktrain, 0.5, 500, n_outputs)\n",
    "minmax = dataset_minmax(banktrain)\n",
    "normalize_dataset(banktrain,minmax)\n",
    "\n",
    "bank = []\n",
    "for row in banktrain:\n",
    "    prediction = predict(network, row)\n",
    "    bank.append(prediction)\n",
    "    #print(\"Prediction :\", prediction)\n",
    "    #print(lala)\n",
    "    \n",
    "print(\"Accuracy :\",accuracy_metric(y,bank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " [[ -1.7713   -10.7665    10.2184    -1.0043  ]\n",
      " [  5.1321    -0.031048   0.32616    1.1151  ]\n",
      " [ -2.0149     3.6874    -1.9385    -3.8918  ]\n",
      " ...\n",
      " [ -1.4377    -1.432      2.1144     0.42067 ]\n",
      " [ -0.89409    3.1991    -1.8219    -2.9452  ]\n",
      " [  6.8248     5.2187    -2.5425     0.5461  ]] \n",
      "\n",
      "Actual Output: \n",
      " [0 1 1 1 1 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 1 0 1 0\n",
      " 0 0 0 0 1 0 1 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
      " 1 1 0 1 1 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1\n",
      " 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 1\n",
      " 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1\n",
      " 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 0 0 1 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0\n",
      " 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1\n",
      " 1 0 1 0 1 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
      " 1 0 1 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0\n",
      " 0 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
      " 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 0 0 1 0 1\n",
      " 1 0 0 1 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1 0\n",
      " 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1\n",
      " 0 1 0 1 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0\n",
      " 1 0 0 0 0 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
      " 1 0 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 0\n",
      " 0 0 1 1 1 1 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 0\n",
      " 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 0\n",
      " 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 1 1 0 1 1 0 1 1 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0] \n",
      "\n",
      "Predicted Output: \n",
      " [[0.71007533]\n",
      " [0.74918711]\n",
      " [0.70365485]\n",
      " [0.35905054]\n",
      " [0.36215724]\n",
      " [0.79120258]\n",
      " [0.43581235]\n",
      " [0.79047787]\n",
      " [0.51053922]\n",
      " [0.67141028]\n",
      " [0.79627814]\n",
      " [0.79483093]\n",
      " [0.57046995]\n",
      " [0.79661431]\n",
      " [0.66395474]\n",
      " [0.64504449]\n",
      " [0.46574684]\n",
      " [0.53809966]\n",
      " [0.76832627]\n",
      " [0.76665949]\n",
      " [0.65666942]\n",
      " [0.73363111]\n",
      " [0.80124024]\n",
      " [0.79031545]\n",
      " [0.78283543]\n",
      " [0.79286641]\n",
      " [0.70888297]\n",
      " [0.56764038]\n",
      " [0.78731853]\n",
      " [0.78850717]\n",
      " [0.52241591]\n",
      " [0.70130947]\n",
      " [0.73567348]\n",
      " [0.50277802]\n",
      " [0.70986596]\n",
      " [0.71329021]\n",
      " [0.79482779]\n",
      " [0.79109513]\n",
      " [0.55005362]\n",
      " [0.79522463]\n",
      " [0.76713006]\n",
      " [0.80519232]\n",
      " [0.8071966 ]\n",
      " [0.43632697]\n",
      " [0.50111847]\n",
      " [0.79396138]\n",
      " [0.57242476]\n",
      " [0.57530492]\n",
      " [0.79712831]\n",
      " [0.80881867]\n",
      " [0.59733871]\n",
      " [0.83370695]\n",
      " [0.74361572]\n",
      " [0.61057442]\n",
      " [0.72453607]\n",
      " [0.80842135]\n",
      " [0.75371908]\n",
      " [0.7949359 ]\n",
      " [0.58016894]\n",
      " [0.79682066]\n",
      " [0.67729011]\n",
      " [0.74247976]\n",
      " [0.6017442 ]\n",
      " [0.7950314 ]\n",
      " [0.7936317 ]\n",
      " [0.79495938]\n",
      " [0.78419444]\n",
      " [0.69910766]\n",
      " [0.77650459]\n",
      " [0.39136284]\n",
      " [0.79677182]\n",
      " [0.78625229]\n",
      " [0.51321267]\n",
      " [0.7553965 ]\n",
      " [0.800664  ]\n",
      " [0.67001228]\n",
      " [0.78158214]\n",
      " [0.53774887]\n",
      " [0.4699472 ]\n",
      " [0.50978822]\n",
      " [0.7764387 ]\n",
      " [0.63566538]\n",
      " [0.64358629]\n",
      " [0.83176093]\n",
      " [0.71606169]\n",
      " [0.72245582]\n",
      " [0.79690459]\n",
      " [0.51958194]\n",
      " [0.67630623]\n",
      " [0.70342999]\n",
      " [0.51628438]\n",
      " [0.50743367]\n",
      " [0.77910946]\n",
      " [0.80889424]\n",
      " [0.67010478]\n",
      " [0.79636228]\n",
      " [0.72339893]\n",
      " [0.75952018]\n",
      " [0.78934642]\n",
      " [0.64392218]\n",
      " [0.77802401]\n",
      " [0.73730875]\n",
      " [0.78969306]\n",
      " [0.78272237]\n",
      " [0.56680342]\n",
      " [0.71415373]\n",
      " [0.79991494]\n",
      " [0.7939164 ]\n",
      " [0.64001838]\n",
      " [0.77544055]\n",
      " [0.79315749]\n",
      " [0.63673179]\n",
      " [0.68851365]\n",
      " [0.49752669]\n",
      " [0.7951544 ]\n",
      " [0.84792312]\n",
      " [0.82673455]\n",
      " [0.82099978]\n",
      " [0.38680554]\n",
      " [0.49897742]\n",
      " [0.73343429]\n",
      " [0.50660852]\n",
      " [0.78660263]\n",
      " [0.7858403 ]\n",
      " [0.62924105]\n",
      " [0.69179434]\n",
      " [0.69907621]\n",
      " [0.78961137]\n",
      " [0.80833695]\n",
      " [0.44528478]\n",
      " [0.82933976]\n",
      " [0.44302579]\n",
      " [0.70354512]\n",
      " [0.64420834]\n",
      " [0.61030231]\n",
      " [0.74093391]\n",
      " [0.78909458]\n",
      " [0.78937993]\n",
      " [0.79766757]\n",
      " [0.57154242]\n",
      " [0.79357405]\n",
      " [0.74627294]\n",
      " [0.77845227]\n",
      " [0.64237651]\n",
      " [0.78895991]\n",
      " [0.71845843]\n",
      " [0.40575161]\n",
      " [0.64678945]\n",
      " [0.63277635]\n",
      " [0.74126754]\n",
      " [0.63979868]\n",
      " [0.57992947]\n",
      " [0.79178009]\n",
      " [0.79199617]\n",
      " [0.35860275]\n",
      " [0.52126865]\n",
      " [0.75788078]\n",
      " [0.79757496]\n",
      " [0.71649639]\n",
      " [0.79880325]\n",
      " [0.78436243]\n",
      " [0.57046995]\n",
      " [0.80052908]\n",
      " [0.74296625]\n",
      " [0.36172373]\n",
      " [0.79766087]\n",
      " [0.63623109]\n",
      " [0.64000968]\n",
      " [0.50745825]\n",
      " [0.52173355]\n",
      " [0.8244241 ]\n",
      " [0.49868851]\n",
      " [0.81746995]\n",
      " [0.76530125]\n",
      " [0.78794762]\n",
      " [0.7459581 ]\n",
      " [0.46016086]\n",
      " [0.78331358]\n",
      " [0.68219294]\n",
      " [0.78989692]\n",
      " [0.72151008]\n",
      " [0.78635192]\n",
      " [0.48276884]\n",
      " [0.7969324 ]\n",
      " [0.42169342]\n",
      " [0.59921376]\n",
      " [0.80164941]\n",
      " [0.79177646]\n",
      " [0.49464997]\n",
      " [0.78070728]\n",
      " [0.62106402]\n",
      " [0.6878541 ]\n",
      " [0.51827207]\n",
      " [0.79107432]\n",
      " [0.78226389]\n",
      " [0.77690156]\n",
      " [0.72108538]\n",
      " [0.5879708 ]\n",
      " [0.57100332]\n",
      " [0.79332951]\n",
      " [0.36591178]\n",
      " [0.62193776]\n",
      " [0.81266078]\n",
      " [0.74564738]\n",
      " [0.58394567]\n",
      " [0.7950413 ]\n",
      " [0.43874554]\n",
      " [0.7902606 ]\n",
      " [0.536933  ]\n",
      " [0.6222822 ]\n",
      " [0.47266523]\n",
      " [0.73894557]\n",
      " [0.69160593]\n",
      " [0.69852209]\n",
      " [0.79586474]\n",
      " [0.78911594]\n",
      " [0.80784003]\n",
      " [0.373023  ]\n",
      " [0.38566544]\n",
      " [0.84774139]\n",
      " [0.60678486]\n",
      " [0.43908899]\n",
      " [0.72536407]\n",
      " [0.79678809]\n",
      " [0.79733308]\n",
      " [0.75723717]\n",
      " [0.39317855]\n",
      " [0.7969256 ]\n",
      " [0.68694709]\n",
      " [0.7616384 ]\n",
      " [0.79388629]\n",
      " [0.7951533 ]\n",
      " [0.7527662 ]\n",
      " [0.68679762]\n",
      " [0.60888059]\n",
      " [0.79633193]\n",
      " [0.49510059]\n",
      " [0.79713739]\n",
      " [0.6174253 ]\n",
      " [0.79514768]\n",
      " [0.72352897]\n",
      " [0.36384895]\n",
      " [0.79427517]\n",
      " [0.7888005 ]\n",
      " [0.60774247]\n",
      " [0.71480678]\n",
      " [0.50015244]\n",
      " [0.81950392]\n",
      " [0.53014128]\n",
      " [0.79727815]\n",
      " [0.83886274]\n",
      " [0.68789442]\n",
      " [0.69530755]\n",
      " [0.70577491]\n",
      " [0.78526403]\n",
      " [0.4873874 ]\n",
      " [0.83138402]\n",
      " [0.49486969]\n",
      " [0.73103075]\n",
      " [0.66902119]\n",
      " [0.49019618]\n",
      " [0.79612054]\n",
      " [0.4495912 ]\n",
      " [0.79567218]\n",
      " [0.35526407]\n",
      " [0.78967331]\n",
      " [0.63266692]\n",
      " [0.64272543]\n",
      " [0.8213569 ]\n",
      " [0.51880988]\n",
      " [0.79224892]\n",
      " [0.76706789]\n",
      " [0.77850597]\n",
      " [0.3815049 ]\n",
      " [0.76622212]\n",
      " [0.76659895]\n",
      " [0.36639798]\n",
      " [0.7955767 ]\n",
      " [0.75639884]\n",
      " [0.78891365]\n",
      " [0.59615826]\n",
      " [0.75097829]\n",
      " [0.67381457]\n",
      " [0.79692955]\n",
      " [0.50463769]\n",
      " [0.59979535]\n",
      " [0.78939626]\n",
      " [0.78604888]\n",
      " [0.81989423]\n",
      " [0.70503062]\n",
      " [0.77310526]\n",
      " [0.79219526]\n",
      " [0.37064057]\n",
      " [0.77455734]\n",
      " [0.73604774]\n",
      " [0.52804935]\n",
      " [0.7942484 ]\n",
      " [0.80998198]\n",
      " [0.74966627]\n",
      " [0.71072154]\n",
      " [0.58239059]\n",
      " [0.69400804]\n",
      " [0.79569611]\n",
      " [0.79142779]\n",
      " [0.7723208 ]\n",
      " [0.77383708]\n",
      " [0.37048691]\n",
      " [0.83486535]\n",
      " [0.80304194]\n",
      " [0.7954834 ]\n",
      " [0.71553094]\n",
      " [0.77291022]\n",
      " [0.79743891]\n",
      " [0.57125063]\n",
      " [0.69942099]\n",
      " [0.79513196]\n",
      " [0.76954296]\n",
      " [0.82852941]\n",
      " [0.51373523]\n",
      " [0.79787842]\n",
      " [0.79500995]\n",
      " [0.79488379]\n",
      " [0.69530438]\n",
      " [0.79766718]\n",
      " [0.6785676 ]\n",
      " [0.78119961]\n",
      " [0.78123542]\n",
      " [0.57596257]\n",
      " [0.79127842]\n",
      " [0.4627522 ]\n",
      " [0.83361268]\n",
      " [0.79311224]\n",
      " [0.78681306]\n",
      " [0.78506991]\n",
      " [0.54735139]\n",
      " [0.79680169]\n",
      " [0.79576089]\n",
      " [0.65781957]\n",
      " [0.78866122]\n",
      " [0.74009537]\n",
      " [0.79487253]\n",
      " [0.69777728]\n",
      " [0.79748957]\n",
      " [0.58642006]\n",
      " [0.62702934]\n",
      " [0.5070896 ]\n",
      " [0.63110563]\n",
      " [0.50801632]\n",
      " [0.77032744]\n",
      " [0.7764713 ]\n",
      " [0.73073779]\n",
      " [0.79737148]\n",
      " [0.50538935]\n",
      " [0.79105331]\n",
      " [0.66902145]\n",
      " [0.59414955]\n",
      " [0.78980155]\n",
      " [0.79574164]\n",
      " [0.52317596]\n",
      " [0.81226246]\n",
      " [0.79565788]\n",
      " [0.82854629]\n",
      " [0.79169953]\n",
      " [0.79136237]\n",
      " [0.83136618]\n",
      " [0.53389037]\n",
      " [0.79063868]\n",
      " [0.46075368]\n",
      " [0.71649639]\n",
      " [0.70667308]\n",
      " [0.51094149]\n",
      " [0.79473979]\n",
      " [0.5356286 ]\n",
      " [0.62106402]\n",
      " [0.74665309]\n",
      " [0.82866932]\n",
      " [0.66083645]\n",
      " [0.68923586]\n",
      " [0.79514958]\n",
      " [0.70665005]\n",
      " [0.78996827]\n",
      " [0.85454087]\n",
      " [0.57046887]\n",
      " [0.78958407]\n",
      " [0.53194356]\n",
      " [0.51147134]\n",
      " [0.40767604]\n",
      " [0.6984776 ]\n",
      " [0.45120344]\n",
      " [0.7908479 ]\n",
      " [0.79705377]\n",
      " [0.55991742]\n",
      " [0.79242278]\n",
      " [0.36926713]\n",
      " [0.76350189]\n",
      " [0.78745911]\n",
      " [0.7964119 ]\n",
      " [0.77236092]\n",
      " [0.38184209]\n",
      " [0.79486963]\n",
      " [0.78922093]\n",
      " [0.70578457]\n",
      " [0.68091952]\n",
      " [0.78760973]\n",
      " [0.75008382]\n",
      " [0.68632214]\n",
      " [0.70238084]\n",
      " [0.36361912]\n",
      " [0.79218305]\n",
      " [0.51096779]\n",
      " [0.78646488]\n",
      " [0.67778601]\n",
      " [0.79619991]\n",
      " [0.79419484]\n",
      " [0.70968153]\n",
      " [0.81749937]\n",
      " [0.50447003]\n",
      " [0.52655981]\n",
      " [0.4875991 ]\n",
      " [0.7968133 ]\n",
      " [0.78193561]\n",
      " [0.71116927]\n",
      " [0.46739129]\n",
      " [0.80918924]\n",
      " [0.77234665]\n",
      " [0.77952759]\n",
      " [0.50509919]\n",
      " [0.49168714]\n",
      " [0.43753376]\n",
      " [0.73416973]\n",
      " [0.79477412]\n",
      " [0.77118329]\n",
      " [0.68128848]\n",
      " [0.73662392]\n",
      " [0.59971942]\n",
      " [0.79937845]\n",
      " [0.7833745 ]\n",
      " [0.71309278]\n",
      " [0.78403995]\n",
      " [0.47507642]\n",
      " [0.54870483]\n",
      " [0.77290029]\n",
      " [0.58572442]\n",
      " [0.74176856]\n",
      " [0.78827612]\n",
      " [0.73735941]\n",
      " [0.4152707 ]\n",
      " [0.82347651]\n",
      " [0.49859287]\n",
      " [0.72347729]\n",
      " [0.78553038]\n",
      " [0.80214356]\n",
      " [0.49687481]\n",
      " [0.41033696]\n",
      " [0.78141067]\n",
      " [0.79777488]\n",
      " [0.79128104]\n",
      " [0.66133491]\n",
      " [0.58075979]\n",
      " [0.51164762]\n",
      " [0.78504567]\n",
      " [0.6656455 ]\n",
      " [0.84431428]\n",
      " [0.78272841]\n",
      " [0.74910968]\n",
      " [0.60160273]\n",
      " [0.51906846]\n",
      " [0.78876385]\n",
      " [0.64588197]\n",
      " [0.72837238]\n",
      " [0.75798101]\n",
      " [0.7937142 ]\n",
      " [0.50739578]\n",
      " [0.80624513]\n",
      " [0.78821804]\n",
      " [0.78610011]\n",
      " [0.35931407]\n",
      " [0.78597019]\n",
      " [0.72709286]\n",
      " [0.78683831]\n",
      " [0.68852013]\n",
      " [0.79424358]\n",
      " [0.66839602]\n",
      " [0.74900443]\n",
      " [0.81054939]\n",
      " [0.64273704]\n",
      " [0.72907125]\n",
      " [0.79686816]\n",
      " [0.79430856]\n",
      " [0.75199302]\n",
      " [0.78437656]\n",
      " [0.78108975]\n",
      " [0.81970373]\n",
      " [0.75225286]\n",
      " [0.50175533]\n",
      " [0.55596998]\n",
      " [0.5030709 ]\n",
      " [0.52320616]\n",
      " [0.78997985]\n",
      " [0.67052903]\n",
      " [0.6117878 ]\n",
      " [0.78428425]\n",
      " [0.71743756]\n",
      " [0.62750619]\n",
      " [0.79286232]\n",
      " [0.79469234]\n",
      " [0.62214381]\n",
      " [0.79455167]\n",
      " [0.823812  ]\n",
      " [0.66902145]\n",
      " [0.72174806]\n",
      " [0.79350343]\n",
      " [0.66833352]\n",
      " [0.77656977]\n",
      " [0.79595392]\n",
      " [0.68852013]\n",
      " [0.50998787]\n",
      " [0.79248271]\n",
      " [0.41128051]\n",
      " [0.77032744]\n",
      " [0.78679669]\n",
      " [0.77006108]\n",
      " [0.80932103]\n",
      " [0.62724504]\n",
      " [0.80724315]\n",
      " [0.40889419]\n",
      " [0.63741485]\n",
      " [0.77687666]\n",
      " [0.60426908]\n",
      " [0.70261433]\n",
      " [0.79626317]\n",
      " [0.79761672]\n",
      " [0.66297729]\n",
      " [0.73245045]\n",
      " [0.50173281]\n",
      " [0.79713023]\n",
      " [0.39011194]\n",
      " [0.35807189]\n",
      " [0.58526928]\n",
      " [0.50269243]\n",
      " [0.69595428]\n",
      " [0.63945511]\n",
      " [0.83930299]\n",
      " [0.51542842]\n",
      " [0.54121234]\n",
      " [0.79514984]\n",
      " [0.68852013]\n",
      " [0.75693376]\n",
      " [0.73603714]\n",
      " [0.44441794]\n",
      " [0.43753376]\n",
      " [0.79537718]\n",
      " [0.51493946]\n",
      " [0.79435447]\n",
      " [0.51471476]\n",
      " [0.47998281]\n",
      " [0.63774143]\n",
      " [0.65702772]\n",
      " [0.50186797]\n",
      " [0.76567599]\n",
      " [0.7849796 ]\n",
      " [0.78364739]\n",
      " [0.50112205]\n",
      " [0.47142168]\n",
      " [0.7733772 ]\n",
      " [0.43134846]\n",
      " [0.41196452]\n",
      " [0.70566061]\n",
      " [0.58685883]\n",
      " [0.8276173 ]\n",
      " [0.79647119]\n",
      " [0.69115555]\n",
      " [0.79765226]\n",
      " [0.66902145]\n",
      " [0.57418224]\n",
      " [0.78830621]\n",
      " [0.62129833]\n",
      " [0.38973395]\n",
      " [0.79670624]\n",
      " [0.76271156]\n",
      " [0.53635562]\n",
      " [0.79187516]\n",
      " [0.50217433]\n",
      " [0.79526515]\n",
      " [0.79670753]\n",
      " [0.60395099]\n",
      " [0.50887852]\n",
      " [0.67797685]\n",
      " [0.79077548]\n",
      " [0.7994707 ]\n",
      " [0.72371761]\n",
      " [0.73477427]\n",
      " [0.78309326]\n",
      " [0.79279373]\n",
      " [0.84170329]\n",
      " [0.76118276]\n",
      " [0.57285039]\n",
      " [0.49048919]\n",
      " [0.7063975 ]\n",
      " [0.79632772]\n",
      " [0.60196862]\n",
      " [0.48893844]\n",
      " [0.79059212]\n",
      " [0.79446911]\n",
      " [0.81256148]\n",
      " [0.82332105]\n",
      " [0.78258365]\n",
      " [0.81773785]\n",
      " [0.68712367]\n",
      " [0.76131512]\n",
      " [0.3635252 ]\n",
      " [0.7132637 ]\n",
      " [0.78076546]\n",
      " [0.7913498 ]\n",
      " [0.79429563]\n",
      " [0.75317332]\n",
      " [0.75494566]\n",
      " [0.75941248]\n",
      " [0.76777087]\n",
      " [0.82202099]\n",
      " [0.79653182]\n",
      " [0.72208574]\n",
      " [0.79033126]\n",
      " [0.53905677]\n",
      " [0.79017715]\n",
      " [0.55701808]\n",
      " [0.73986417]\n",
      " [0.79040738]\n",
      " [0.3667418 ]\n",
      " [0.81991069]\n",
      " [0.74752134]\n",
      " [0.8005492 ]\n",
      " [0.69022672]\n",
      " [0.80231749]\n",
      " [0.79399473]\n",
      " [0.75509612]\n",
      " [0.79163946]\n",
      " [0.78786828]\n",
      " [0.71045091]\n",
      " [0.66586072]\n",
      " [0.54352791]\n",
      " [0.78940383]\n",
      " [0.7386662 ]\n",
      " [0.6892674 ]\n",
      " [0.53220832]\n",
      " [0.67384969]\n",
      " [0.79537279]\n",
      " [0.50662955]\n",
      " [0.36560259]\n",
      " [0.77706008]\n",
      " [0.5832924 ]\n",
      " [0.48905978]\n",
      " [0.79261279]\n",
      " [0.76361939]\n",
      " [0.79572499]\n",
      " [0.63370991]\n",
      " [0.54681632]\n",
      " [0.64337077]\n",
      " [0.80244523]\n",
      " [0.55956106]\n",
      " [0.79205859]\n",
      " [0.83646069]\n",
      " [0.79312227]\n",
      " [0.83902534]\n",
      " [0.51570707]\n",
      " [0.79739756]\n",
      " [0.50155745]\n",
      " [0.78278741]\n",
      " [0.78010602]\n",
      " [0.79468504]\n",
      " [0.76151604]\n",
      " [0.48803255]\n",
      " [0.78683305]\n",
      " [0.67490821]\n",
      " [0.73386691]\n",
      " [0.48572257]\n",
      " [0.80959108]\n",
      " [0.79098956]\n",
      " [0.64258178]\n",
      " [0.49002143]\n",
      " [0.78319048]\n",
      " [0.66047561]\n",
      " [0.7946881 ]\n",
      " [0.79713499]\n",
      " [0.7905111 ]\n",
      " [0.78640398]\n",
      " [0.52160188]\n",
      " [0.46602053]\n",
      " [0.77357911]\n",
      " [0.78964501]\n",
      " [0.58136534]\n",
      " [0.79312853]\n",
      " [0.5829877 ]\n",
      " [0.3639421 ]\n",
      " [0.62228273]\n",
      " [0.64725621]\n",
      " [0.79534499]\n",
      " [0.79518516]\n",
      " [0.66902145]\n",
      " [0.67890958]\n",
      " [0.78076108]\n",
      " [0.51743871]\n",
      " [0.7450868 ]\n",
      " [0.52959807]\n",
      " [0.79722368]\n",
      " [0.66832275]\n",
      " [0.48735975]\n",
      " [0.49481981]\n",
      " [0.79574586]\n",
      " [0.35783543]\n",
      " [0.51128595]\n",
      " [0.75292534]\n",
      " [0.76189482]\n",
      " [0.7857879 ]\n",
      " [0.78903491]\n",
      " [0.77345955]\n",
      " [0.79518677]\n",
      " [0.78462958]\n",
      " [0.79318954]\n",
      " [0.79524362]\n",
      " [0.79428801]\n",
      " [0.55547196]\n",
      " [0.39344388]\n",
      " [0.77231866]\n",
      " [0.50853128]\n",
      " [0.38613034]\n",
      " [0.41412541]\n",
      " [0.78014736]\n",
      " [0.52285432]\n",
      " [0.58366457]\n",
      " [0.79506516]\n",
      " [0.75637896]\n",
      " [0.56942054]\n",
      " [0.60063259]\n",
      " [0.72543321]\n",
      " [0.77425628]\n",
      " [0.78498465]\n",
      " [0.79041287]\n",
      " [0.81437863]\n",
      " [0.79199227]\n",
      " [0.7932652 ]\n",
      " [0.74433442]\n",
      " [0.72108516]\n",
      " [0.39658142]\n",
      " [0.74844831]\n",
      " [0.44005251]\n",
      " [0.56531462]\n",
      " [0.55225073]\n",
      " [0.78005138]\n",
      " [0.75502694]\n",
      " [0.41297247]\n",
      " [0.78691985]\n",
      " [0.74204532]\n",
      " [0.35701437]\n",
      " [0.78384377]\n",
      " [0.78313818]\n",
      " [0.72749731]\n",
      " [0.7565842 ]\n",
      " [0.83237226]\n",
      " [0.52328193]\n",
      " [0.77537742]\n",
      " [0.70562109]\n",
      " [0.49093873]\n",
      " [0.79634996]\n",
      " [0.37135455]\n",
      " [0.74872085]\n",
      " [0.52827859]\n",
      " [0.81763091]\n",
      " [0.62373796]\n",
      " [0.55991742]\n",
      " [0.65479826]\n",
      " [0.81873555]\n",
      " [0.54635053]\n",
      " [0.79058217]\n",
      " [0.76011319]\n",
      " [0.61135475]\n",
      " [0.74301541]\n",
      " [0.61639414]\n",
      " [0.77693242]\n",
      " [0.55081734]\n",
      " [0.79670324]\n",
      " [0.51472793]\n",
      " [0.79371177]\n",
      " [0.78647016]\n",
      " [0.71389773]\n",
      " [0.77742506]\n",
      " [0.77717589]\n",
      " [0.37166105]\n",
      " [0.73076725]\n",
      " [0.69376845]\n",
      " [0.78340872]\n",
      " [0.74175717]\n",
      " [0.79576464]\n",
      " [0.56688283]\n",
      " [0.52736433]\n",
      " [0.69955323]\n",
      " [0.79044716]\n",
      " [0.42955754]\n",
      " [0.82963478]\n",
      " [0.74297085]\n",
      " [0.5960659 ]\n",
      " [0.47339047]\n",
      " [0.78215953]\n",
      " [0.40901347]\n",
      " [0.49809244]\n",
      " [0.70725477]\n",
      " [0.65006427]\n",
      " [0.8321567 ]\n",
      " [0.76317673]\n",
      " [0.50173424]\n",
      " [0.37166105]\n",
      " [0.79562687]\n",
      " [0.80118997]\n",
      " [0.63847022]\n",
      " [0.77424181]\n",
      " [0.78754843]\n",
      " [0.59967227]\n",
      " [0.76553109]\n",
      " [0.56513092]\n",
      " [0.79460882]\n",
      " [0.78184799]\n",
      " [0.79585264]\n",
      " [0.74487367]\n",
      " [0.78500781]\n",
      " [0.79655742]\n",
      " [0.59532437]\n",
      " [0.69976844]\n",
      " [0.78685782]\n",
      " [0.65554184]\n",
      " [0.59388534]\n",
      " [0.63939248]\n",
      " [0.78863166]\n",
      " [0.72606388]\n",
      " [0.72911902]\n",
      " [0.63575369]\n",
      " [0.76461022]\n",
      " [0.49631838]\n",
      " [0.57084933]\n",
      " [0.81583979]\n",
      " [0.80868784]\n",
      " [0.83392638]\n",
      " [0.49119888]\n",
      " [0.60197783]\n",
      " [0.82126603]\n",
      " [0.66262279]\n",
      " [0.57841522]\n",
      " [0.78863501]\n",
      " [0.58447823]\n",
      " [0.7573694 ]\n",
      " [0.79090221]\n",
      " [0.83217102]\n",
      " [0.79296123]\n",
      " [0.52432349]\n",
      " [0.58069359]\n",
      " [0.78625673]\n",
      " [0.67107043]\n",
      " [0.78306654]\n",
      " [0.63114615]\n",
      " [0.78592546]\n",
      " [0.73839869]\n",
      " [0.57426738]\n",
      " [0.75456067]\n",
      " [0.61946876]\n",
      " [0.78544743]\n",
      " [0.35800299]\n",
      " [0.78487385]\n",
      " [0.81991216]\n",
      " [0.69655658]\n",
      " [0.54902222]\n",
      " [0.82437847]\n",
      " [0.6353935 ]\n",
      " [0.61197892]\n",
      " [0.79141212]\n",
      " [0.52103581]\n",
      " [0.79708669]\n",
      " [0.50365382]\n",
      " [0.74994408]\n",
      " [0.62151148]\n",
      " [0.52821529]\n",
      " [0.49874117]\n",
      " [0.69489039]\n",
      " [0.56303679]\n",
      " [0.54713174]\n",
      " [0.65825556]\n",
      " [0.84466037]\n",
      " [0.65348454]\n",
      " [0.68238887]\n",
      " [0.79667059]\n",
      " [0.6450334 ]\n",
      " [0.79716276]\n",
      " [0.75711484]\n",
      " [0.55116619]\n",
      " [0.51541124]\n",
      " [0.79463103]\n",
      " [0.7950615 ]\n",
      " [0.57250187]\n",
      " [0.75236954]\n",
      " [0.71234933]\n",
      " [0.5078394 ]\n",
      " [0.48199769]\n",
      " [0.788189  ]\n",
      " [0.79724085]\n",
      " [0.85210162]\n",
      " [0.50788182]\n",
      " [0.81812114]\n",
      " [0.80148899]\n",
      " [0.78776986]\n",
      " [0.79678715]\n",
      " [0.65174108]\n",
      " [0.73303389]\n",
      " [0.75115694]\n",
      " [0.79335449]\n",
      " [0.46057161]\n",
      " [0.73449602]\n",
      " [0.5165881 ]\n",
      " [0.76710941]\n",
      " [0.79587283]\n",
      " [0.78949292]\n",
      " [0.79780755]\n",
      " [0.78098781]\n",
      " [0.79559357]\n",
      " [0.52205147]\n",
      " [0.7898189 ]\n",
      " [0.7249523 ]\n",
      " [0.79613552]\n",
      " [0.50442891]\n",
      " [0.67914427]\n",
      " [0.74458181]\n",
      " [0.68873113]\n",
      " [0.4994618 ]\n",
      " [0.78679562]\n",
      " [0.35766204]\n",
      " [0.73411173]\n",
      " [0.70075402]\n",
      " [0.78482869]\n",
      " [0.4930003 ]\n",
      " [0.66908918]\n",
      " [0.79112348]\n",
      " [0.78843104]\n",
      " [0.75520026]\n",
      " [0.5320746 ]\n",
      " [0.68541297]\n",
      " [0.48982779]\n",
      " [0.80512705]\n",
      " [0.60710404]\n",
      " [0.79465914]\n",
      " [0.79629615]\n",
      " [0.78466489]\n",
      " [0.75413377]\n",
      " [0.78715012]\n",
      " [0.79756578]\n",
      " [0.56338743]\n",
      " [0.7204426 ]\n",
      " [0.73731781]\n",
      " [0.50961113]\n",
      " [0.66393417]\n",
      " [0.67358403]\n",
      " [0.5229266 ]\n",
      " [0.79685469]\n",
      " [0.78711994]] \n",
      "\n",
      "Loss: \n",
      " 0.3227307156886346\n",
      "\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       232\n",
      "           1       0.98      0.99      0.99       180\n",
      "\n",
      "    accuracy                           0.99       412\n",
      "   macro avg       0.99      0.99      0.99       412\n",
      "weighted avg       0.99      0.99      0.99       412\n",
      "\n",
      "Confusion matrix :\n",
      " [[229   3]\n",
      " [  1 179]] \n",
      "\n",
      "Accuracy :\n",
      " 0.9902912621359223\n"
     ]
    }
   ],
   "source": [
    "x2 = test_features\n",
    "y2 = test_labels\n",
    "\n",
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        #parameters\n",
    "        self.inputSize = 4\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "        \n",
    "        self.v = np.random.randn(self.inputSize, self.hiddenSize) \n",
    "        self.w = np.random.randn(self.hiddenSize, self.outputSize) \n",
    "        \n",
    "    def forward(self, x2):\n",
    "        #forward propagation through our network\n",
    "        self.z = np.dot(x2, self.v) # dot product of X (input) and first set of 4x3 weights\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = np.dot(self.z2, self.w) # dot product of hidden layer (z2) and second set of 3x1 weights\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o \n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        # activation function \n",
    "        return 1/(1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        #derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, x2, y, o):\n",
    "        # backward propgate through the network\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error*self.sigmoidPrime(o) # applying derivative of sigmoid to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.w.T) # z2 error: how much our hidden layer weights contributed to output error\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.z2) # applying derivative of sigmoid to z2 error\n",
    "\n",
    "        self.v += x.T.dot(self.z2_delta) # adjusting first set (input --> hidden) weights\n",
    "        \n",
    "    def train (self, x2, y):\n",
    "        o = self.forward(x)\n",
    "        self.backward(x, y, o)\n",
    "        \n",
    "NN = Neural_Network()\n",
    "for i in range(1):# trains the NN 1x times\n",
    "    print (\"Input: \\n\" ,str(x2),\"\\n\") \n",
    "    print (\"Actual Output: \\n\" ,str(y),\"\\n\") \n",
    "    print (\"Predicted Output: \\n\" ,str(NN.forward(x)),\"\\n\") \n",
    "    print (\"Loss: \\n\" ,str(np.mean(np.square(y - NN.forward(x2))))) # mean sum squared loss\n",
    "    print (\"\\n\")\n",
    "    #NN.train(x2, y)\n",
    "    \n",
    "print(\"Classification report : \\n\",classification_report(test_labels, predicted_labels)) \n",
    "\n",
    "print(\"Confusion matrix :\\n\",confusion_matrix(test_labels, predicted_labels),\"\\n\") \n",
    "\n",
    "print(\"Accuracy :\\n\",accuracy_score(test_labels, predicted_labels)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
